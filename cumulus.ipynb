{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from PIL import Image\n",
    "from os import getcwd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "input_dir = getcwd() + '/img/input_samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#                                                      Array utils                                                        #\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def normalize(arr, zero_padding=False, offset_coef=0.001):\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    offset = offset_coef * abs(min_val)\n",
    "    new_arr = arr - min_val if zero_padding else arr - min_val + offset\n",
    "    d = (max_val - min_val) if zero_padding else (max_val - min_val + offset)\n",
    "    new_arr = new_arr / d\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def normalize_img(arr):\n",
    "    arr_min = np.min(arr)\n",
    "    arr_max = np.max(arr)\n",
    "    if arr_min > 0 and arr_max < 255: \n",
    "        return arr\n",
    "    else:\n",
    "        arr_norm = 255 * (arr - arr_min) / (arr_max - arr_min)\n",
    "        return arr_norm\n",
    "\n",
    "\n",
    "def threshold_arr_1d(arr, th_val, use_abs=False):\n",
    "    th_val = abs(th_val) if use_abs else th_val\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] < th_val:\n",
    "            arr[i] = th_val\n",
    "    return arr\n",
    "    \n",
    "    \n",
    "def threshold_arr_2d(arr, th_val, use_abs=False):\n",
    "    for i in range(arr.shape[0]):\n",
    "        arr[i, :] = threshold_arr_1d(arr[i, :], th_val, use_abs)\n",
    "    return arr\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "#                                                       FFT utils                                                         #\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def find_ft_1d(img):\n",
    "    ft = np.fft.fft(img)\n",
    "    return np.fft.fftshift(ft)\n",
    "\n",
    "\n",
    "def find_ift_1d(ft):\n",
    "    ift = np.fft.ifftshift(ft)\n",
    "    return np.fft.ifft(ift)\n",
    "\n",
    "\n",
    "def find_ft_2d(img):\n",
    "    ft = np.fft.fft2(img)\n",
    "    return np.fft.fftshift(ft)\n",
    "\n",
    "\n",
    "def find_ift_2d(ft):\n",
    "    ift = np.fft.ifftshift(ft)\n",
    "    return np.fft.ifft2(ift)\n",
    "\n",
    "\n",
    "def freq_numbers_1d(size):\n",
    "    if size % 2:\n",
    "        return np.arange(-(size // 2), size // 2 + 1, 1) \n",
    "    else:\n",
    "        return np.arange(-(size // 2), size // 2, 1) \n",
    "    \n",
    "\n",
    "def freq_arr_1d(size, step=1):\n",
    "    freq = freq_numbers_1d(size) \n",
    "    return freq / step / size\n",
    "\n",
    "\n",
    "def freq_numbers_2d(size):\n",
    "    x_size, y_size = size\n",
    "    x_freq_numbers = freq_numbers_1d(x_size)\n",
    "    y_freq_numbers = freq_numbers_1d(y_size)\n",
    "    return x_freq_numbers, y_freq_numbers\n",
    "    \n",
    "\n",
    "def freq_arr_2d(size, x_step=1, y_step=1):\n",
    "    x_size, y_size = size\n",
    "    x_freq_numbers, y_freq_numbers = freq_numbers_2d(size) \n",
    "    x_freq = x_freq_numbers / x_step / x_size\n",
    "    y_freq = y_freq_numbers / y_step / y_size\n",
    "    return x_freq, y_freq\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "#                                                   Frequency domain filters                                              #\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def freq_filter_2d(x_freq, y_freq):\n",
    "    x, y = np.meshgrid(x_freq, y_freq)\n",
    "    f = np.hypot(x, y)\n",
    "    return f\n",
    "\n",
    "\n",
    "# should be deprecated or changed\n",
    "\"\"\"\n",
    "def freq_filter(x_freq, y_freq, factor=2.4):\n",
    "    eps = 10 ** -8\n",
    "    x, y = np.meshgrid(x_freq, y_freq)\n",
    "    f = np.hypot(x, y)\n",
    "    f = f ** factor + eps\n",
    "    return normalize(1 / f)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def freq_pink_filter_2d(x_freq, y_freq, factor=1, zero_padding=False, no_mean=False):\n",
    "    x, y = np.meshgrid(x_freq, y_freq)\n",
    "    f = np.hypot(x, y)\n",
    "    f = 1 / (1 + np.abs(f))\n",
    "    f = f ** factor\n",
    "    f = np.where(f==1, 0, f) if no_mean else f\n",
    "    return normalize(f, zero_padding)\n",
    "\n",
    "\n",
    "def freq_pink_filter_1d(x_freq, factor=1, no_mean=False):\n",
    "    f = 1 / (1 + np.abs(x_freq))\n",
    "    f = f ** factor\n",
    "    f = np.where(f==1, 0, f) if no_mean else f\n",
    "    return normalize(f, zero_padding)\n",
    "\n",
    "\n",
    "# replaced by no_mean flag in freq_filter_1d\n",
    "\"\"\"\n",
    "def freq_filter_1d_a(x_freq, factor=1):\n",
    "    f = 1 / np.where(x_freq == 0, 1, np.abs(x_freq))\n",
    "    f = f ** factor\n",
    "    f[len(x_freq) // 2] = 0\n",
    "    return f\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def freq_sharp_round_filter_2d(x_freq, y_freq, radius, low_pass_filter=True):\n",
    "    f = np.ones((np.size(x_freq), np.size(y_freq)))\n",
    "    check = gt if low_pass_filter else lt\n",
    "    rr = radius ** 2\n",
    "\n",
    "    for i, x in enumerate(x_freq):\n",
    "        for j, y in enumerate(y_freq):\n",
    "            if check(x ** 2 + y ** 2, rr):\n",
    "                f[i, j] = 0\n",
    "    return f\n",
    "\n",
    "\n",
    "def freq_sharp_square_filter(x_freq, y_freq, width, angle=0, low_pass_filter=True):\n",
    "    f = np.ones((np.size(x_freq), np.size(y_freq)))\n",
    "    check = np.greater if low_pass_filter else np.less\n",
    "    angle_radians = math.radians(angle)\n",
    "    rotation_matrix = np.array([[math.cos(angle_radians), -math.sin(angle_radians)],\n",
    "                                [math.sin(angle_radians), math.cos(angle_radians)]])\n",
    "    \n",
    "    for i, x in enumerate(x_freq):\n",
    "        for j, y in enumerate(y_freq):\n",
    "            rotated_x, rotated_y = np.dot(rotation_matrix, np.array([x, y])) \n",
    "\n",
    "            if check(abs(rotated_x) + abs(rotated_y), width):\n",
    "                f[i, j] = 0\n",
    "    return f\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "#                                                Spatial domain utils (NOT REWORKED)                                      #\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def spatial_smooth_filter(x_size, y_size, depth, horiz=True):\n",
    "    values = np.linspace(0, 1, depth)\n",
    "    values = 6 * values ** 5 - 15 * values ** 4 + 10 * values ** 3\n",
    "    values = 1 - values\n",
    "    if horiz:\n",
    "        kernel = np.tile(values, (y_size, 1))\n",
    "    else:\n",
    "        kernel = values[:, np.newaxis] * np.ones((1, x_size))   \n",
    "    return kernel\n",
    "\n",
    "\n",
    "def make_img_transition_x(img, depth, is_dx_pos=True):\n",
    "    y_size, x_size = img.shape\n",
    "    additional_img = gen_cloud(x_size + depth, y_size)   \n",
    "    transition_kernel = spatial_smooth_filter(x_size, y_size, depth)     \n",
    "    \n",
    "    new_img = np.copy(img)\n",
    "    if is_dx_pos:\n",
    "        new_img[:, -depth:x_size] = img[:, -depth:x_size] * transition_kernel + \\\n",
    "                                additional_img[:, 0:depth] * (1 - transition_kernel)\n",
    "        return new_img, additional_img[:, depth:]    \n",
    "    else:\n",
    "        transition_kernel = np.fliplr(transition_kernel)\n",
    "        new_img[:, 0:depth] = img[:, 0:depth] * transition_kernel + \\\n",
    "                          additional_img[:, -depth:] * (1 - transition_kernel)  \n",
    "        return new_img, additional_img[:, 0:-depth]    \n",
    "\n",
    "\n",
    "def make_img_transition_y(img, depth, is_dy_pos=True):\n",
    "    y_size, x_size = img.shape\n",
    "    additional_img = gen_cloud(x_size, y_size + depth)   \n",
    "    transition_kernel = spatial_smooth_filter(x_size, y_size, depth, horiz=False)\n",
    "        \n",
    "    new_img = np.copy(img)\n",
    "    if is_dy_pos:\n",
    "        new_img[-depth:x_size, :] = img[-depth:x_size, :] * transition_kernel + \\\n",
    "                                additional_img[0:depth, :] * (1 - transition_kernel)\n",
    "        return new_img, additional_img[depth:, :]    \n",
    "    else:\n",
    "        transition_kernel = np.flipud(transition_kernel)\n",
    "        new_img[0:depth, :] = img[0:depth, :] * transition_kernel + \\\n",
    "                          additional_img[-depth:, :] * (1 - transition_kernel)  \n",
    "        return new_img, additional_img[0:-depth:1, :]    \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "#                                                       Other (NOT REWORKED)                                              #\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def normalize_psd(original_magn, other_magn):\n",
    "    original_psd = original_magn ** 2\n",
    "    other_psd = other_magn ** 2\n",
    "    return (np.sum(original_psd) / np.sum(other_psd)) ** 0.5\n",
    "\n",
    "\n",
    "def fit_clement(x_freq, y_freq, alpha1, alpha2, eta=0.1, angle=3):\n",
    "    x, y = np.meshgrid(x_freq, y_freq)\n",
    "    f = np.hypot(x, y )\n",
    "    fp = abs(angle * x  + y)\n",
    "    f = np.sqrt((1 - eta) * f ** 2 + eta * fp ** 2)    \n",
    "    f = (1 + abs(f)) ** (-alpha1) + 0.02 * (1 + abs(f)) ** (-alpha2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def surrogates(x, ns, tol_pc=5., verbose=True, maxiter=1E6, sorttype=\"quicksort\"):\n",
    "    # as per the steps given in Lancaster et al., Phys. Rep (2018)\n",
    "    nx = x.shape[0]\n",
    "    xs = np.zeros((ns, nx))\n",
    "    maxiter = 10000\n",
    "    ii = np.arange(nx)\n",
    "\n",
    "    # get the fft of the original array\n",
    "    x_amp = np.abs(np.fft.fft(x))\n",
    "    x_srt = np.sort(x)\n",
    "    r_orig = np.argsort(x)\n",
    "\n",
    "    # loop over surrogate number\n",
    "    pb_fmt = \"{desc:<5.5}{percentage:3.0f}%|{bar:30}{r_bar}\"\n",
    "    pb_desc = \"Estimating IAAFT surrogates ...\"\n",
    "    for k in tqdm(range(ns), bar_format=pb_fmt, desc=pb_desc,\n",
    "                  disable=not verbose):\n",
    "\n",
    "        # 1) Generate random shuffle of the data\n",
    "        count = 0\n",
    "        r_prev = np.random.permutation(ii)\n",
    "        r_curr = r_orig\n",
    "        z_n = x[r_prev]\n",
    "        percent_unequal = 100.\n",
    "\n",
    "        # core iterative loop\n",
    "        while (percent_unequal > tol_pc) and (count < maxiter):\n",
    "            r_prev = r_curr\n",
    "\n",
    "            # 2) FFT current iteration yk, and then invert it but while\n",
    "            # replacing the amplitudes with the original amplitudes but\n",
    "            # keeping the angles from the FFT-ed version of the random\n",
    "            y_prev = z_n\n",
    "            fft_prev = np.fft.fft(y_prev)\n",
    "            phi_prev = np.angle(fft_prev)\n",
    "            e_i_phi = np.exp(phi_prev * 1j)\n",
    "            z_n = np.fft.ifft(x_amp * e_i_phi)\n",
    "\n",
    "            # 3) rescale zk to the original distribution of x\n",
    "            r_curr = np.argsort(z_n, kind=sorttype)\n",
    "            z_n[r_curr] = x_srt.copy()\n",
    "            percent_unequal = ((r_curr != r_prev).sum() * 100.) / nx\n",
    "\n",
    "            # 4) repeat until number of unequal entries between r_curr and \n",
    "            # r_prev is less than tol_pc percent\n",
    "            count += 1\n",
    "\n",
    "        if count >= (maxiter - 1):\n",
    "            print(\"maximum number of iterations reached!\")\n",
    "\n",
    "        xs[k] = np.real(z_n)\n",
    "\n",
    "    return xs\n",
    "\n",
    "\n",
    "def adjust_freq_1d(img):\n",
    "    return np.append(img, abs(img[0]))\n",
    "\n",
    "\n",
    "def adjust_img_1d(img):\n",
    "    return np.append(img, img[0])\n",
    "        \n",
    "        \n",
    "def gen_cloud(x_size, y_size, factor=2.4):\n",
    "    xx = np.linspace(-x_size / 2, x_size / 2, x_size)\n",
    "    yy = np.linspace(-y_size / 2, y_size / 2, y_size)\n",
    "    whitenoise = np.random.normal(0, 1, (y_size, x_size))\n",
    "    cloud_freq = find_ft_2d(whitenoise)  \n",
    "    kernel = freq_filter(xx, yy, factor=factor)\n",
    "    cloud_freq_filtered = cloud_freq * kernel\n",
    "    cloud_spatial = find_ift_2d(cloud_freq_filtered).real\n",
    "    return normalize_img(cloud_spatial)\n",
    "\n",
    "\n",
    "# Reworked\n",
    "def show_images(*images, vrange=None, x_fig_size=10, y_fig_size=10, cmap='gray', graphs_per_row=2):\n",
    "    row_num = len(images) // graphs_per_row + 1 if len(images) % graphs_per_row else len(images) // graphs_per_row\n",
    "    col_num = len(images) // row_num + 1 if len(images) % row_num else len(images) // row_num\n",
    "    images_len = len(images)\n",
    "    \n",
    "    f, axes = plt.subplots(row_num, col_num, figsize=(x_fig_size, y_fig_size))  \n",
    "    if row_num == 1 and col_num == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    for index, ax in enumerate(axes.flatten()):\n",
    "        if index < images_len:\n",
    "            if vrange:\n",
    "                vmin, vmax = vrange\n",
    "                im = ax.imshow(images[index], cmap=cmap, vmin=vmin, vmax=vmax, aspect='equal')\n",
    "                \n",
    "            else:\n",
    "                im = ax.imshow(images[index], cmap=cmap, aspect='equal')\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            plt.colorbar(im, cax=cax)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return f, axes\n",
    "\n",
    "    \n",
    "def show_surfaces(*surfaces, axes=None, cmap=None, colorscale='Thermal', showscale=True):  \n",
    "    if cmap:\n",
    "        cmin, cmax = cmap\n",
    "    else:\n",
    "        if axes:\n",
    "            z_data = [surf[-1] for surf in surfaces]\n",
    "        else:\n",
    "            z_data = surfaces\n",
    "        cmin, cmax = np.min(z_data), np.max(z_data)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for surf in surfaces:\n",
    "        if axes: \n",
    "            x, y = axes\n",
    "            z = surf\n",
    "        else:\n",
    "            x, y, z = surf\n",
    "        fig.add_trace(go.Surface(x=x, y=y, z=z, cmin=cmin, cmax=cmax, colorscale=colorscale, showscale=showscale))\n",
    "        showscale = False\n",
    "    return fig\n",
    "    \n",
    "\n",
    "def lin_regression(x, y):\n",
    "    # y - original img\n",
    "    # x - restored img\n",
    "    num = np.mean(x * y) - np.mean(x) * np.mean(y)\n",
    "    denum = np.mean(x ** 2) - np.mean(x) ** 2\n",
    "    a = num / denum\n",
    "    b = np.mean(y) - a * np.mean(x)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def lin_phase(start, end, size):\n",
    "    pos_freq = np.linspace(start, end, size // 2)\n",
    "    neg_freq = -pos_freq[::-1]\n",
    "    return np.append(neg_freq, pos_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496154c",
   "metadata": {},
   "source": [
    "### Cumulus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f05274",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(input_dir + '2.jpg').convert('L')\n",
    "img -= np.mean(img)\n",
    "img_fr = find_ft_2d(img)\n",
    "\n",
    "x_size, y_size = img.shape\n",
    "x_shift = x_size // 2\n",
    "y_shift = y_size // 2\n",
    "xx = freq_numbers_1d(x_size)\n",
    "yy = freq_numbers_1d(y_size)\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "x_ax_size = 10\n",
    "y_ax_size = 8.5\n",
    "\n",
    "magn_raw = np.abs(img_fr)\n",
    "magn = normalize(magn_raw)\n",
    "magn_factor = normalize_psd(magn_raw, magn)\n",
    "phase = np.angle(img_fr)\n",
    "restored_img = find_ift_2d(magn_factor * magn * np.exp(1j * phase)).real\n",
    "\n",
    "f, ax = show_images(img, np.log10(magn), phase, restored_img, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5472c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_fr = find_ft_2d(phase)\n",
    "phase_magn = np.abs(phase_fr)\n",
    "phase_angle = np.angle(phase_fr)\n",
    "restored_phase = find_ift_2d(phase_magn * np.exp(1j * phase_angle)).real\n",
    "\n",
    "f, ax = show_images(np.log10(phase_magn), phase_angle, restored_phase, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e184ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = show_surfaces(np.log10(magn), axes=(xx, yy))\n",
    "# f_1.update_layout(scene = dict(zaxis = dict(range=[-5, 0])))\n",
    "# f_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_2 = show_surfaces(phase[x_size // 2 - 10: x_size // 2 + 10, y_size // 2 - 10: y_size // 2 + 10], axes=(list(range(20)), list(range(20))))\n",
    "f_2 = show_surfaces(phase, axes=(xx, yy))\n",
    "# f_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa94a0",
   "metadata": {},
   "source": [
    "### Generating pseudo random phase trying to mimic same phase distribution along each axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6add8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_phase_y = np.zeros((x_size, y_size))\n",
    "for i in range(y_size):\n",
    "    new_phase_y[i, :] = surrogates(phase[i, :], 1)\n",
    "    \n",
    "new_phase_x = np.zeros((x_size, y_size))\n",
    "for i in range(x_size):\n",
    "    new_phase_x[:, i] = surrogates(phase[:, i], 1)\n",
    "\n",
    "# New phase and 1/f kernel\n",
    "new_phase_sur = new_phase_x + new_phase_y \n",
    "\n",
    "# Restoring image using original phase and generated phase\n",
    "restored_img = find_ift_2d(magn_factor * magn * np.exp(1j * new_phase_sur)).real\n",
    "\n",
    "f, ax = show_images(new_phase_sur, restored_img, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3214f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring image using 1/f kernel for magnitude and new phase \n",
    "# Results are equal to regular FFT syntesis\n",
    "\n",
    "kernel = freq_pink_filter_2d(xx, yy)\n",
    "kernel_magn_factor = normalize_psd(magn_raw, kernel)\n",
    "restored_img = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * new_phase_sur)).real\n",
    "\n",
    "f, ax = show_images(np.log10(kernel), restored_img, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7ab7c",
   "metadata": {},
   "source": [
    "### Restoring image using linearly changing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2053c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_phase = [np.pi / 3, 2 * np.pi / 3, np.pi, np.pi / 2, 3 * np.pi / 2]\n",
    "initial_phase += [-i for i in initial_phase]\n",
    "whitenoise = np.random.normal(0, 1, (y_size, x_size))\n",
    "whitenoise_fr = find_ft_2d(whitenoise)\n",
    "lin_random_phase = np.angle(whitenoise_fr)\n",
    "\n",
    "width = 5\n",
    "scale = 0.2\n",
    "alpha_x = 1.4\n",
    "alpha_y = 1.\n",
    "step = 1\n",
    "\n",
    "for i in range(-width, width + 1, step):\n",
    "    initial_phase_x = random.choice(initial_phase)\n",
    "    initial_phase_y = random.choice(initial_phase)\n",
    "    freq_y = lin_phase(initial_phase_y, initial_phase_y + random.choice(initial_phase), y_size)\n",
    "    freq_x = lin_phase(initial_phase_x, initial_phase_x + random.choice(initial_phase), x_size)\n",
    "    lin_random_phase[lin_random_phase.shape[0] // 2 + i, :] = freq_y\n",
    "    lin_random_phase[:, lin_random_phase.shape[1] // 2 + i] = freq_x\n",
    "\n",
    "restored_img_fr = magn_factor * magn * np.exp(1j * lin_random_phase)\n",
    "restored_img = find_ift_2d(restored_img_fr).real\n",
    "\n",
    "f, ax = show_images(lin_random_phase, restored_img, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "magn_factor = normalize_psd(magn_raw, kernel)\n",
    "restored_img_fr = magn_factor * kernel * np.exp(1j * lin_random_phase)\n",
    "restored_img = find_ift_2d(restored_img_fr).real\n",
    "\n",
    "f, ax = show_images(lin_random_phase, restored_img, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a71127",
   "metadata": {},
   "source": [
    "### 1D regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom regression formulas\n",
    "\n",
    "# y_log = np.log(y_vals)\n",
    "# x_log = np.log(1 + x_vals)\n",
    "\n",
    "# alpha = -2 * np.sum(y_log * x_log) / np.sum(x_log ** 2) \n",
    "# alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b684ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation along Y-axis\n",
    "\n",
    "alpha = 1.\n",
    "dy = 1\n",
    "factor_y = [1, 0.2, 0.2, 0.15, 0.25, 0.08]\n",
    "const_y = [0.0019, 0.0008, 0.0015, 0.001, 0.0012, 0.0008]\n",
    "\n",
    "y_vals = magn[x_size // 2 + dy, magn.shape[1] // 2:] \n",
    "x_vals = np.array(range(len(y_vals)))\n",
    "\n",
    "w_y = factor_y[abs(dy)] * ((1 + abs(x_vals)) ** (-alpha) + const_y[abs(dy)])\n",
    "# kernel = freq_pink_filter_2d(xx, yy, factor=1)\n",
    "# kernel[magn.shape[0] // 2, :] = w\n",
    "\n",
    "plt.plot(x_vals, w_y)\n",
    "plt.plot(x_vals[1:], y_vals[1:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c579ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation along X-axis\n",
    "\n",
    "alpha = 1.45\n",
    "dx = 1\n",
    "factor_x = [1, 1, 1, 1, 1, 1]\n",
    "const_x = [0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015]\n",
    "\n",
    "y_vals = magn[magn.shape[0] // 2:, y_size // 2 + dx] \n",
    "x_vals = np.array(range(len(y_vals)))\n",
    "w_x = factor_x[abs(dx)] * ((1 + abs(x_vals)) ** (-alpha) + const_x[abs(dx)])\n",
    "\n",
    "plt.plot(x_vals, w_x)\n",
    "plt.plot(x_vals[0:], y_vals[0:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76681da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation along Y = X\n",
    "\n",
    "alpha1 = 1.8\n",
    "alpha2 = 1.0\n",
    "\n",
    "l = magn.shape[0] // 2\n",
    "y_vals = np.zeros(l)\n",
    "\n",
    "for i in range(l):\n",
    "    y_vals[i] = magn[l + i, l + i]\n",
    "x_vals = np.array(range(len(y_vals))) * 2 ** 0.5\n",
    "\n",
    "y_log = np.log10(y_vals)\n",
    "x_log = np.log10(1 + x_vals)\n",
    "\n",
    "w_diag = (1 + abs(x_vals)) ** (-alpha1) + 0.02 * (1 + abs(x_vals)) ** (-alpha2)\n",
    "# kernel[:, magn.shape[1] // 2] = w\n",
    "\n",
    "plt.plot(x_vals, w_diag)\n",
    "plt.plot(x_vals[1:], y_vals[1:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06465fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined kernel with previous approximation results\n",
    "\n",
    "width = 5\n",
    "scale = 0.2\n",
    "alpha_x = 1.4\n",
    "alpha_y = 1.\n",
    "step = 1\n",
    "\n",
    "# kernel = freq_pink_filter_2d(xx, yy, factor=0.5)\n",
    "kernel = fit_clement(xx, yy, alpha1, alpha2)\n",
    "# kernel = np.zeros((x_size, y_size))\n",
    "\n",
    "for i in range(-width, width + 1, step):\n",
    "    w_y = factor_y[abs(i)] * ((1 + abs(yy)) ** (-alpha_y) + const_y[abs(i)])\n",
    "    w_x = factor_x[abs(i)] * ((1 + abs(xx)) ** (-alpha_x) + const_x[abs(i)])\n",
    "    kernel[magn.shape[0] // 2 + i, :] = w_y\n",
    "    kernel[:, magn.shape[1] // 2 + i] = w_x\n",
    "    \n",
    "# kernel[magn.shape[0] // 2, :] = w_y \n",
    "# kernel[:, magn.shape[1] // 2] = w_x \n",
    "kernel = normalize(kernel)\n",
    "\n",
    "f = show_surfaces(np.log10(kernel), axes=(xx, yy))\n",
    "# fig.update_layout(scene=dict(zaxis = dict(range=[-5, 0])))\n",
    "# f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc218db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restring image using approximated kernel and original phase\n",
    "restored_img_1 = find_ift_2d(kernel * np.exp(1j * phase)).real\n",
    "a, b = lin_regression(restored_img_1, img)\n",
    "restored_img_1 = a * restored_img_1 + b\n",
    "\n",
    "# Restring image using approximated kernel and random phase\n",
    "whitenoise = np.random.normal(0, 1, (y_size, x_size))\n",
    "random_phase = np.angle(find_ft_2d(whitenoise))\n",
    "kernel_magn_factor = normalize_psd(magn_raw, kernel)\n",
    "restored_img_2 = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * random_phase)).real\n",
    "\n",
    "# Restring image using approximated kernel and linear phase\n",
    "restored_img_3 = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * lin_random_phase)).real\n",
    "\n",
    "# plt.imshow(restored_img_1, cmap='gray', vmin=np.min(img), vmax=np.max(img))\n",
    "f, ax = show_images(restored_img_1, restored_img_2, restored_img_3, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2545dad",
   "metadata": {},
   "source": [
    "### 2D regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = magn\n",
    "x_vals = freq_filter_2d(xx, yy)\n",
    "\n",
    "y_log = np.log10(y_vals)\n",
    "x_log = np.log10(1 + x_vals)\n",
    "\n",
    "alpha = -2 * np.sum(y_log * x_log) / np.sum(x_log ** 2) \n",
    "kernel = freq_pink_filter_2d(xx, yy, factor=alpha / 2)\n",
    "print(f\"alpha = {alpha / 2}\")\n",
    "\n",
    "f = show_surfaces(np.log10(kernel), np.log10(magn), axes=(xx, yy))\n",
    "# f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring image using approximated kernel and original phase \n",
    "kernel_magn_factor = normalize_psd(magn_raw, kernel)\n",
    "restored_image_1 = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * phase)).real\n",
    "a, b = lin_regression(restored_image_1, img)\n",
    "restored_image_1 = a * restored_image_1 + b\n",
    "\n",
    "# Restoring image using approximated kernel and random phase \n",
    "restored_image_2 = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * random_phase)).real\n",
    "\n",
    "# Restoring image using approximated kernel and linear phase \n",
    "restored_image_3 = find_ift_2d(kernel_magn_factor * kernel * np.exp(1j * lin_random_phase)).real\n",
    "\n",
    "f, ax = show_images(restored_image_1, restored_image_2, restored_image_3, vrange=(np.min(img), np.max(img)), \n",
    "                    x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698d8cd",
   "metadata": {},
   "source": [
    "### New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d01e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr, y_arr = np.meshgrid(xx, yy)\n",
    "window = normalize(1 - np.sqrt(x_arr ** 2 + y_arr ** 2))\n",
    "window = window\n",
    "\n",
    "windowed_img = window * img\n",
    "window_ft = find_ft_2d(window)\n",
    "windowed_img_ft = find_ft_2d(windowed_img)\n",
    "  \n",
    "magn_windowed = np.abs(windowed_img_ft)\n",
    "phase_windowed = np.angle(windowed_img_ft)\n",
    "restored_img = find_ift_2d(magn_windowed * np.exp(1j * phase_windowed)).real\n",
    "\n",
    "vmin = np.min(windowed_img)\n",
    "vmax = np.max(windowed_img)\n",
    "\n",
    "f, ax = show_images(windowed_img, np.log10(magn_windowed), phase_windowed, restored_img, \n",
    "                    x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_img_ft = find_ft_2d(phase_windowed)\n",
    "m = np.abs(windowed_img_ft)\n",
    "a = np.angle(windowed_img_ft)\n",
    "\n",
    "whitenoise = np.random.normal(0, 1, (x_size, y_size))\n",
    "psd = m * m\n",
    "restored_phase = find_ift_2d(psd ** 0.5 * np.abs(whitenoise) * np.exp(1j * a)).real\n",
    "\n",
    "kernel = freq_pink_filter_2d(xx, yy, factor=0.5)\n",
    "cloud_hf = find_ift_2d(magn_windowed * np.exp(1j * restored_phase)).real\n",
    "a, b = lin_regression(cloud_hf, img)\n",
    "cloud_hf = a * cloud_hf + b\n",
    "\n",
    "f, ax = show_images(np.log10(m), restored_phase, cloud_hf, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 1.8\n",
    "alpha2 = 1.0\n",
    "alpha_x = 1.4\n",
    "alpha_y = 1.\n",
    "\n",
    "whitenoise = np.random.normal(0, 1, (x_size, y_size))\n",
    "whitenoise_fr = find_ft_2d(whitenoise)\n",
    "kernel = fit_clement(xx, yy, alpha1, alpha2, eta=0.5)\n",
    "cloud_lf = find_ift_2d(whitenoise_fr * kernel).real\n",
    "a, b = lin_regression(cloud_lf, img)\n",
    "cloud_lf = a * cloud_lf + b\n",
    "\n",
    "f, ax = show_images(np.log10(kernel), cloud_hf + 0.001 * cloud_lf, x_fig_size=x_ax_size, y_fig_size=y_ax_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0242ca7",
   "metadata": {},
   "source": [
    "### Defining an angle between X-axis and ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_exp = 2\n",
    "threshold = 9\n",
    "width_x = 5\n",
    "width_y = 0\n",
    "\n",
    "# Removing lines along X-axis to get better angle estimation \n",
    "min_magn = np.min(magn_windowed)\n",
    "fit_magn = np.log(magn_windowed / min_magn)\n",
    "\n",
    "for w in range(-width_y, width_y):\n",
    "    fit_magn[:, y_shift + w] = fit_magn[:, y_shift + width_y] * 0\n",
    "\n",
    "for w in range(-width_x, width_x):\n",
    "    fit_magn[x_shift + w, :] = fit_magn[x_shift + width_x, :]\n",
    "\n",
    "# Clipping magnitude \n",
    "fit_magn = np.where(fit_magn < threshold, 0, fit_magn)\n",
    "fit_magn_max = np.exp(np.max(fit_magn))\n",
    "fit_magn = fit_magn / np.mean(fit_magn)\n",
    "\n",
    "# Defining slope with regression\n",
    "avg_xy = -np.sum(x * y * fit_magn ** weight_exp)\n",
    "avg_y2 = np.sum(y * y * fit_magn ** weight_exp)\n",
    "avg_z2 = np.sum(fit_magn ** weight_exp)\n",
    "a = avg_xy / avg_y2\n",
    "angle = np.arctan(a) * 180 / np.pi\n",
    "\n",
    "# Defining STD along the lines with the found slopes\n",
    "lperp2 = np.sum((x + a * y) ** 2 * fit_magn ** weight_exp) / avg_z2\n",
    "lpara2 = np.sum((a * x - y) ** 2 * fit_magn ** weight_exp) / avg_z2\n",
    "lperp = lperp2 ** 0.5\n",
    "lpara = lpara2 ** 0.5\n",
    "\n",
    "# Outputing results \n",
    "print(f'A = {a}')\n",
    "print(f'Angle = {angle}')\n",
    "print(f'Slope = {1 / a}')\n",
    "print(f'l_perp = {lperp}')\n",
    "print(f'l_para = {lpara}')\n",
    "print(f'l_perp corrected = {lperp2 ** 0.5}')\n",
    "\n",
    "# Defining lines with found slopes\n",
    "y_line = np.linspace(50, 850)\n",
    "x_line = -a * (y_line - y_shift) + x_shift\n",
    "\n",
    "# Defining gaussian approximation of clipped magnitude\n",
    "model_magn = np.exp(-(x + a * y) ** 2 / 2 / (lperp2)  -(a * x - y) ** 2 / 2 / (lpara2))\n",
    "model_magn = model_magn / np.mean(model_magn)\n",
    "\n",
    "# Plotting \n",
    "f, ax = show_images(fit_magn, model_magn)\n",
    "ax[0].plot(x_line, y_line)\n",
    "ax[1].plot(x_line, y_line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776c3ad",
   "metadata": {},
   "source": [
    "### Regression along lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31339d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel line\n",
    "slope = -1 / a\n",
    "x_line_para = np.linspace(0, img.shape[0], img.shape[0], endpoint=False)\n",
    "y_line_para = slope * (x_line_para - img.shape[0] // 2) + img.shape[1] // 2\n",
    "\n",
    "# Clipping arrays\n",
    "x_mask = (x_line_para < x_size) & (x_line_para > 0)\n",
    "x_line_para = x_line_para[x_mask]\n",
    "y_line_para = y_line_para[x_mask]\n",
    "\n",
    "y_mask = (y_line_para < y_size) & (y_line_para > 0)\n",
    "x_line_para = x_line_para[y_mask]\n",
    "y_line_para = y_line_para[y_mask]\n",
    "\n",
    "# Making arrays to have the same shape as original array\n",
    "x_line_min, x_line_max = int(np.min(x_line_para)), int(np.max(x_line_para))\n",
    "x_line_para = np.linspace(x_line_min, x_line_max, img.shape[0], endpoint=False)\n",
    "y_line_para = slope * (x_line_para - img.shape[0] // 2) + img.shape[1] // 2\n",
    "\n",
    "\n",
    "# Perpendicular line \n",
    "slope = -1 / slope\n",
    "x_line_perp = np.linspace(0, img.shape[0], img.shape[0], endpoint=False)\n",
    "y_line_perp = slope * (x_line_perp - img.shape[0] // 2) + img.shape[1] // 2\n",
    "\n",
    "# Clipping arrays\n",
    "x_mask = (x_line_perp < x_size) & (x_line_perp > 0)\n",
    "x_line_perp = x_line_perp[x_mask]\n",
    "y_line_perp = y_line_perp[x_mask]\n",
    "\n",
    "y_mask = (y_line_perp < y_size) & (y_line_perp > 0)\n",
    "x_line_perp = x_line_perp[y_mask]\n",
    "y_line_perp = y_line_perp[y_mask]\n",
    "\n",
    "x_line_min, x_line_max = int(np.min(x_line_perp)) + 1, int(np.max(x_line_perp))\n",
    "x_line_perp = np.linspace(x_line_min, x_line_max, img.shape[0], endpoint=False)\n",
    "y_line_perp = slope * (x_line_perp - img.shape[0] // 2) + img.shape[1] // 2\n",
    "\n",
    "f, ax = show_images(np.log10(magn_windowed), cmap='gray', \n",
    "                    x_fig_size=x_ax_size/2, y_fig_size=y_ax_size/2)\n",
    "ax[0].plot(x_line_para, y_line_para)\n",
    "ax[0].plot(x_line_perp, y_line_perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c11cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D regression along Y = angle * X line\n",
    "x_indexes_para = x_line_para.astype(int)[x_shift:]\n",
    "y_indexes_para = y_line_para.astype(int)[y_shift:]\n",
    "\n",
    "x_vals_para = np.hypot(x_indexes_para - x_shift, y_indexes_para - y_shift)\n",
    "y_vals_para = magn_windowed[y_indexes_para, x_indexes_para]\n",
    "y_vals_para = normalize(y_vals_para)\n",
    "\n",
    "# Regression variables\n",
    "x_log_para = np.log(1 + x_vals_para)\n",
    "y_log_para = np.log(y_vals_para)\n",
    "alpha_para = -2 * np.sum(y_log_para * x_log_para) / np.sum(x_log_para ** 2) \n",
    "alpha_para = alpha_para / 2 \n",
    "\n",
    "para_approx = (1 + x_vals_para) ** (-alpha_para)\n",
    "para_approx /= np.max(para_approx)\n",
    "\n",
    "plt.plot(x_vals_para, y_vals_para)\n",
    "plt.plot(x_vals_para + 1, para_approx)\n",
    "plt.yscale('log')\n",
    "\n",
    "print(alpha_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7792b",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretching gaussain along axes\n",
    "# Define the size of the kernel and standard deviation\n",
    "kernel_size = 21\n",
    "sigma = 2.0\n",
    "\n",
    "# Create a standard 2D Gaussian kernel\n",
    "x = np.linspace(-kernel_size // 2, kernel_size // 2, kernel_size)\n",
    "y = np.linspace(-kernel_size // 2, kernel_size // 2, kernel_size)\n",
    "x, y = np.meshgrid(x, y)\n",
    "kernel = np.exp(-(x**2 + y**2) / (2.0 * sigma**2))\n",
    "\n",
    "# Create a scaling factor for stretching along y = x\n",
    "stretching_factor = 2.0  # Adjust this value to control the stretching\n",
    "new_x = stretching_factor * x\n",
    "\n",
    "# Create the stretched Gaussian kernel\n",
    "stretched_kernel = np.exp(-(new_x**2 + y**2) / (2.0 * sigma**2))\n",
    "\n",
    "# Display the original and stretched kernels\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(kernel, cmap='viridis')\n",
    "plt.title('Original Gaussian Kernel')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(stretched_kernel, cmap='viridis')\n",
    "plt.title('Stretched Gaussian Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c83697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some experiments\n",
    "# Blending whitenoise phase with original phase\n",
    "\n",
    "whitenoise = np.random.normal(0, 1, (x_size, y_size))\n",
    "whitenoise_fr = find_ft_2d(whitenoise)\n",
    "new_angle = np.angle(whitenoise_fr)\n",
    "\n",
    "width = 25 #img.shape[0] // 2 - 1\n",
    "step = 1\n",
    "\n",
    "for i in range(-width, width + 1, step):\n",
    "    new_angle[img.shape[0] // 2 + i, :] = phase_windowed[img.shape[0] // 2 + i, :]\n",
    "    \n",
    "for j in range(-width, width + 1, step):\n",
    "    new_angle[:, img.shape[1] // 2 + j] = phase_windowed[:, img.shape[1] // 2 + j]\n",
    "        \n",
    "        \n",
    "dunno = find_ift_2d(magn_windowed * np.exp(1j * new_angle)).real \n",
    "# f, ax = show_images(dunno / window)\n",
    "\n",
    "# sharp_filter = 1 - (freq_sharp_round_filter_2d(xx, yy, radius=900) - freq_sharp_round_filter_2d(xx, yy, radius=50))\n",
    "# dunno = find_ift_2d(sharp_filter * magn_windowed * np.exp(1j * phase_windowed)).real\n",
    "dunno = find_ift_2d(magn_windowed * np.exp(1j * new_angle)).real\n",
    "a, b = lin_regression(dunno, img)\n",
    "dunno = a * dunno + b\n",
    "\n",
    "f, ax = show_images(new_angle, dunno)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
